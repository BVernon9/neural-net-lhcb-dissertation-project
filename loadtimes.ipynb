{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Data:\n",
      " Training set: [('equalisation_N030_A7', 'VP0-1'), ('equalisation_M59_A4', 'VP0-0'), ('equalisation_M98_A1', 'VP0-1'), ('equalisation_N030_A7', 'VP3-2'), ('equalisation_M98_A1', 'VP2-0')]\n",
      " 115 evaluation sets to test.\n",
      "First: ('equalisation_M98_A1', 'VP0-0') Last: ('equalisation_N013_A10', 'VP3-2')\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# COMMON FUNCTIONS\n",
    "# =======================\n",
    "import numpy as np\n",
    "import tarfile\n",
    "\n",
    "def loadTGZ(tgz, fPath, dtype, skip_header=False):\n",
    "    \"\"\"\n",
    "    Load a CSV file from the tarfile 'tgz' given its file path 'fPath'.  \n",
    "    Optionally skip the header row (if skip_header=True). \n",
    "    Reshape the data into 256*256 and cast to the specified dtype.\n",
    "    \"\"\"\n",
    "    with tgz.extractfile(fPath) as csvfile:\n",
    "        data = np.genfromtxt(csvfile, delimiter=\",\", skip_header=1 if skip_header else 0)\n",
    "        if data.size != 256*256:\n",
    "            raise ValueError(f\"Data size mismatch in {fPath}: expected {256*256}, got {data.size}\")\n",
    "        return data.reshape(256*256).astype(dtype)\n",
    "\n",
    "def removeMaskNaN(outSet):\n",
    "    \"\"\"Remove entries with NaN in any file and entries with mask != 0.\"\"\"\n",
    "    aMask = (outSet[\"mask\"] == 0)\n",
    "    for k in outSet.keys():\n",
    "        aMask &= ~np.isnan(outSet[k])\n",
    "    for k in outSet.keys():\n",
    "        outSet[k] = outSet[k][aMask]\n",
    "    return outSet\n",
    "\n",
    "def normSet(outSet):\n",
    "    \"\"\"\n",
    "    Normalize the variables in a consistent way (using a sensor-specific offset)\n",
    "    to allow comparisons between sensors.\n",
    "    \"\"\"\n",
    "    offset = np.mean(outSet[\"tMean0\"]).astype(np.float16)\n",
    "    outSet[\"tMean0\"] = (outSet[\"tMean0\"] - offset) / 75.\n",
    "    outSet[\"tMeanF\"] = (outSet[\"tMeanF\"] - (offset + 150)) / 75.\n",
    "    outSet[\"tWidth0\"] = (outSet[\"tWidth0\"] - 10) / 7.5\n",
    "    outSet[\"tWidthF\"] = (outSet[\"tWidthF\"] - 10) / 7.5\n",
    "    return outSet\n",
    "\n",
    "# =======================\n",
    "# CELL 1: OLD DATA LOADING\n",
    "# =======================\n",
    "tgzFileName = \"ASideFirstModules.tgz\"\n",
    "\n",
    "# Define module training list for old data\n",
    "moduleFiles = [\n",
    "    \"equalisation_M98_A1\",\n",
    "    \"equalisation_M94_A2\",\n",
    "    \"equalisation_M96_A3\",\n",
    "    \"equalisation_M59_A4\",\n",
    "    \"equalisation_N22_A5\",\n",
    "    \"equalisation_M116_A6\",\n",
    "    \"equalisation_N030_A7\",\n",
    "    \"equalisation_M90_A8\",\n",
    "    \"equalisation_N029_A9\",\n",
    "    \"equalisation_N013_A10\"\n",
    "]\n",
    "\n",
    "# Define training set (to be excluded from evaluation)\n",
    "trainingSet = [\n",
    "    (\"equalisation_N030_A7\",\"VP0-1\"), #offset = 2\n",
    "    (\"equalisation_M59_A4\",\"VP0-0\"), #offset = 1\n",
    "    (\"equalisation_M98_A1\",\"VP0-1\"), #offset = 0\n",
    "    (\"equalisation_N030_A7\",\"VP3-2\"), #offset = -1\n",
    "    (\"equalisation_M98_A1\",\"VP2-0\") #offset = -2\n",
    "]\n",
    "\n",
    "vpList = [f\"VP{i}-{j}\" for i in range(4) for j in range(3)]\n",
    "evaluationSet = [(f, vp) for f in moduleFiles for vp in vpList]\n",
    "evaluationSet = [s for s in evaluationSet if s not in trainingSet]\n",
    "\n",
    "print(f\"Old Data:\\n Training set: {trainingSet}\\n {len(evaluationSet)} evaluation sets to test.\")\n",
    "print(\"First:\", evaluationSet[0], \"Last:\", evaluationSet[-1])\n",
    "\n",
    "def extractFromTGZ(tgzName, dSet):\n",
    "    \"\"\"\n",
    "    Extract the old data files for a given dataset dSet.\n",
    "    dSet is a tuple: (module_name, vp), e.g. (\"equalisation_M98_A1\", \"VP0-1\").\n",
    "    \"\"\"\n",
    "    path = dSet[0] + \"/\"\n",
    "    outSet = {}\n",
    "    with tarfile.open(tgzName, 'r:gz') as tgz:\n",
    "        outSet[\"tMean0\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Trim0_Noise_Mean.csv\", np.float16)\n",
    "        outSet[\"tMeanF\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_TrimF_Noise_Mean.csv\", np.float16)\n",
    "        outSet[\"tWidth0\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Trim0_Noise_Width.csv\", np.float16)\n",
    "        outSet[\"tWidthF\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_TrimF_Noise_Width.csv\", np.float16)\n",
    "        outSet[\"mask\"]    = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Matrix_Mask.csv\", np.float16)\n",
    "        outSet[\"trim\"]    = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Matrix_Trim.csv\", np.int8)\n",
    "    return removeMaskNaN(outSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.1 ms ± 3.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'XT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#XT = tf.convert_to_tensor(X)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#tf.compat.v1.disable_eager_execution()\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Or if you want to time prediction:\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloadModel.predict(XT, verbose=0)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\darre\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2456\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2454\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2456\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2458\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2459\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2460\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\darre\\anaconda3\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1185\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1184\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1185\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m timer\u001b[38;5;241m.\u001b[39mtimeit(number)\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1187\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\darre\\anaconda3\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:173\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    171\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner(it, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'XT' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Keras network used as front end to Tensorflow\n",
    "# See https://keras.io/ for details of the package\n",
    "# import Keras overall\n",
    "import keras\n",
    "# input normalization layer [fixes input variables to useful range]\n",
    "from keras.layers import BatchNormalization\n",
    "# a single NN layer of type \"Dense\" i.e. all inputs connected to all outputs\n",
    "from keras.layers import Dense\n",
    "# The input layer, takes x and starts NN processing\n",
    "from keras.layers import Input\n",
    "# Keras functional methods for defining a NN model\n",
    "from keras.models import Model\n",
    "import time  # Import time module\n",
    "import keras_tuner as kt\n",
    "from keras.saving import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "def adjacent_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom accuracy metric that accepts predictions within ±1 of the true class.\n",
    "    Args:\n",
    "        y_true: True labels (integer values).\n",
    "        y_pred: Predicted probabilities for each class (softmax output).\n",
    "    Returns:\n",
    "        Accuracy metric allowing adjacent values.\n",
    "    \"\"\"\n",
    "    import tensorflow.keras.backend as K\n",
    "\n",
    "    # Get the predicted class (argmax of softmax output)\n",
    "    y_pred_classes = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    # Ensure both tensors are of the same type\n",
    "    y_true = K.cast(y_true, dtype='float32')  # Cast y_true to float32\n",
    "    y_pred_classes = K.cast(y_pred_classes, dtype='float32')  # Cast y_pred_classes to float32 \n",
    "\n",
    "    # Check if predictions are within ±1 of the true labels\n",
    "    correct = K.abs(y_true - y_pred_classes) <= 1\n",
    "\n",
    "    # Calculate mean accuracy\n",
    "    return K.mean(K.cast(correct, dtype='float32'))\n",
    "\n",
    "# Define training data set, slot post then ASIC\n",
    "custom_objects = {\"adjacent_accuracy\": adjacent_accuracy}\n",
    "loadFile = f\"NN_{trainingSet[0][0]}_{trainingSet[0][1]}.keras\"\n",
    "loadModel = keras.saving.load_model(loadFile, custom_objects=custom_objects)\n",
    "%timeit loadModel = keras.saving.load_model(loadFile, custom_objects=custom_objects)\n",
    "\n",
    "import random\n",
    "\n",
    "random_eval_sets = random.sample(evaluationSet, 1)  #select random datasets\n",
    "eval_set = random_eval_sets[0]\n",
    "\n",
    "# Extract and process the evaluation set\n",
    "dSet = extractFromTGZ(tgzFileName, eval_set)\n",
    "dSet = removeMaskNaN(dSet)\n",
    "dSet = normSet(dSet)\n",
    "\n",
    "# Create the input feature matrix and target vector\n",
    "X = np.column_stack([dSet[\"tMean0\"], dSet[\"tMeanF\"],\n",
    "                     dSet[\"tWidth0\"], dSet[\"tWidthF\"]])\n",
    "y = dSet[\"trim\"]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#XT = tf.convert_to_tensor(X)\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Now, measure evaluation time using %timeit:\n",
    "##%timeit loadModel.evaluate(X, y, verbose=0)\n",
    "\n",
    "# Or if you want to time prediction:\n",
    "%timeit loadModel.predict(XT, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
