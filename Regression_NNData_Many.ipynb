{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Data:\n",
      " Training set: [('equalisation_N030_A7', 'VP0-1'), ('equalisation_M59_A4', 'VP0-0'), ('equalisation_M98_A1', 'VP0-1'), ('equalisation_N030_A7', 'VP3-2'), ('equalisation_M98_A1', 'VP2-0')]\n",
      " 115 evaluation sets to test.\n",
      "First: ('equalisation_M98_A1', 'VP0-0') Last: ('equalisation_N013_A10', 'VP3-2')\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# COMMON FUNCTIONS\n",
    "# =======================\n",
    "import numpy as np\n",
    "import tarfile\n",
    "\n",
    "def loadTGZ(tgz, fPath, dtype, skip_header=False):\n",
    "    \"\"\"\n",
    "    Load a CSV file from the tarfile 'tgz' given its file path 'fPath'.  \n",
    "    Optionally skip the header row (if skip_header=True). \n",
    "    Reshape the data into 256*256 and cast to the specified dtype.\n",
    "    \"\"\"\n",
    "    with tgz.extractfile(fPath) as csvfile:\n",
    "        data = np.genfromtxt(csvfile, delimiter=\",\", skip_header=1 if skip_header else 0)\n",
    "        if data.size != 256*256:\n",
    "            raise ValueError(f\"Data size mismatch in {fPath}: expected {256*256}, got {data.size}\")\n",
    "        return data.reshape(256*256).astype(dtype)\n",
    "\n",
    "def removeMaskNaN(outSet):\n",
    "    \"\"\"Remove entries with NaN in any file and entries with mask != 0.\"\"\"\n",
    "    aMask = (outSet[\"mask\"] == 0)\n",
    "    for k in outSet.keys():\n",
    "        aMask &= ~np.isnan(outSet[k])\n",
    "    for k in outSet.keys():\n",
    "        outSet[k] = outSet[k][aMask]\n",
    "    return outSet\n",
    "\n",
    "def normSet(outSet):\n",
    "    \"\"\"\n",
    "    Normalize the variables in a consistent way (using a sensor-specific offset)\n",
    "    to allow comparisons between sensors.\n",
    "    \"\"\"\n",
    "    offset = np.mean(outSet[\"tMean0\"]).astype(np.float16)\n",
    "    outSet[\"tMean0\"] = (outSet[\"tMean0\"] - offset) / 75.\n",
    "    outSet[\"tMeanF\"] = (outSet[\"tMeanF\"] - (offset + 150)) / 75.\n",
    "    outSet[\"tWidth0\"] = (outSet[\"tWidth0\"] - 10) / 7.5\n",
    "    outSet[\"tWidthF\"] = (outSet[\"tWidthF\"] - 10) / 7.5\n",
    "    return outSet\n",
    "\n",
    "# =======================\n",
    "# CELL 1: OLD DATA LOADING\n",
    "# =======================\n",
    "tgzFileName = \"ASideFirstModules.tgz\"\n",
    "\n",
    "# Define module training list for old data\n",
    "moduleFiles = [\n",
    "    \"equalisation_M98_A1\",\n",
    "    \"equalisation_M94_A2\",\n",
    "    \"equalisation_M96_A3\",\n",
    "    \"equalisation_M59_A4\",\n",
    "    \"equalisation_N22_A5\",\n",
    "    \"equalisation_M116_A6\",\n",
    "    \"equalisation_N030_A7\",\n",
    "    \"equalisation_M90_A8\",\n",
    "    \"equalisation_N029_A9\",\n",
    "    \"equalisation_N013_A10\"\n",
    "]\n",
    "\n",
    "# Define training set (to be excluded from evaluation)\n",
    "trainingSet = [\n",
    "    (\"equalisation_N030_A7\",\"VP0-1\"), #offset = 2\n",
    "    (\"equalisation_M59_A4\",\"VP0-0\"), #offset = 1\n",
    "    (\"equalisation_M98_A1\",\"VP0-1\"), #offset = 0\n",
    "    (\"equalisation_N030_A7\",\"VP3-2\"), #offset = -1\n",
    "    (\"equalisation_M98_A1\",\"VP2-0\") #offset = -2\n",
    "]\n",
    "\n",
    "vpList = [f\"VP{i}-{j}\" for i in range(4) for j in range(3)]\n",
    "evaluationSet = [(f, vp) for f in moduleFiles for vp in vpList]\n",
    "evaluationSet = [s for s in evaluationSet if s not in trainingSet]\n",
    "\n",
    "print(f\"Old Data:\\n Training set: {trainingSet}\\n {len(evaluationSet)} evaluation sets to test.\")\n",
    "print(\"First:\", evaluationSet[0], \"Last:\", evaluationSet[-1])\n",
    "\n",
    "def extractFromTGZ(tgzName, dSet):\n",
    "    \"\"\"\n",
    "    Extract the old data files for a given dataset dSet.\n",
    "    dSet is a tuple: (module_name, vp), e.g. (\"equalisation_M98_A1\", \"VP0-1\").\n",
    "    \"\"\"\n",
    "    path = dSet[0] + \"/\"\n",
    "    outSet = {}\n",
    "    with tarfile.open(tgzName, 'r:gz') as tgz:\n",
    "        outSet[\"tMean0\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Trim0_Noise_Mean.csv\", np.float16)\n",
    "        outSet[\"tMeanF\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_TrimF_Noise_Mean.csv\", np.float16)\n",
    "        outSet[\"tWidth0\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Trim0_Noise_Width.csv\", np.float16)\n",
    "        outSet[\"tWidthF\"] = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_TrimF_Noise_Width.csv\", np.float16)\n",
    "        outSet[\"mask\"]    = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Matrix_Mask.csv\", np.float16)\n",
    "        outSet[\"trim\"]    = loadTGZ(tgz, path + f\"Module0_{dSet[1]}_Matrix_Trim.csv\", np.int8)\n",
    "    return removeMaskNaN(outSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Operational Data:\n",
      " 12 evaluation sets to test.\n",
      "First: ('Module25', 'VP0-0') Last: ('Module25', 'VP3-2')\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# CELL 2: NEW OPERATIONAL DATA LOADING\n",
    "# =======================\n",
    "tgzFileName_new = \"Module25_20230621_trim.tgz\"\n",
    "\n",
    "# For the new data, the module prefix is fixed as \"Module25\".\n",
    "evaluationSet_new = [(\"Module25\", vp) for vp in vpList]\n",
    "\n",
    "print(f\"\\nNew Operational Data:\\n {len(evaluationSet_new)} evaluation sets to test.\")\n",
    "print(\"First:\", evaluationSet_new[0], \"Last:\", evaluationSet_new[-1])\n",
    "\n",
    "def extractFromTGZ_new(tgzName, dSet) :\n",
    "    \"\"\"\n",
    "    Extract the new operational data files for a given dataset dSet.\n",
    "    dSet is a tuple: (module_prefix, vp), e.g. (\"Module25\", \"VP0-0\").\n",
    "    For noise files, we skip the header row; for matrix files, we do not.\n",
    "    \"\"\"\n",
    "    module_prefix, vp = dSet\n",
    "    outSet = {}\n",
    "    with tarfile.open(tgzName, 'r:gz') as tgz:\n",
    "        # Noise files (skip header)\n",
    "        outSet[\"tMean0\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_Trim0_Noise_Mean_2023-06-21_14-06-30.csv\", np.float16, skip_header=True)\n",
    "        outSet[\"tMeanF\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_TrimF_Noise_Mean_2023-06-21_14-06-30.csv\", np.float16, skip_header=True)\n",
    "        outSet[\"tWidth0\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_Trim0_Noise_Width_2023-06-21_14-06-30.csv\", np.float16, skip_header=True)\n",
    "        outSet[\"tWidthF\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_TrimF_Noise_Width_2023-06-21_14-06-30.csv\", np.float16, skip_header=True)\n",
    "        # Matrix files (do not skip header)\n",
    "        outSet[\"mask\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_Matrix_Mask_2023-06-21_14-06-30.csv\", np.float16, skip_header=False)\n",
    "        outSet[\"trim\"] = loadTGZ(tgz, f\"{module_prefix}_{vp}_Matrix_Trim_2023-06-21_14-06-30.csv\", np.int8, skip_header=False)\n",
    "    return removeMaskNaN(outSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Now plot the data to see what we have as a training set\n",
    "\n",
    "def plotCorr(ax,trim,val,ytitle):\n",
    "    xBins = np.arange(-0.5,15.5,1)\n",
    "    yBins = np.linspace(np.min(val),np.max(val),100)\n",
    "    ax.hist2d(trim,val,bins=[xBins,yBins])\n",
    "    ax.set_xlabel('Final Trim')\n",
    "    ax.set_ylabel(ytitle)\n",
    "\n",
    "def drawSet(dSet, title='Dataset', norm=False):\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize=(14,3))\n",
    "    fig.suptitle(title)\n",
    "    # final Trim\n",
    "    ax1.hist(dSet[\"trim\"],bins=np.arange(-0.5,15.5))\n",
    "    ax1.set_xlabel('Final Trim')\n",
    "    # mean0 & F\n",
    "    if norm :\n",
    "        bins = np.linspace(-2,2,128)\n",
    "    else :\n",
    "        bins = np.linspace(-0.5,2047.5,128)\n",
    "    ax2.hist(dSet[\"tMean0\"], bins=bins, label = \"0 Mean\")\n",
    "    ax2.legend()\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Mean 0')\n",
    "    ax3.hist(dSet[\"tMeanF\"], bins=bins, label = \"F Mean\")\n",
    "    ax3.legend()\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set_xlabel('Mean F')\n",
    "    # widths\n",
    "    if norm :\n",
    "        bins = np.linspace(-2,2,128)\n",
    "    else :\n",
    "        bins = np.linspace(-0.5,25.5, 26)\n",
    "    ax4.hist(dSet[\"tWidth0\"], bins=bins, label = \"0 Width\")\n",
    "    ax4.legend()\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.set_xlabel('Width 0')\n",
    "    ax5.hist(dSet[\"tWidthF\"], bins=bins, label = \"F Width\")\n",
    "    ax5.legend()\n",
    "    ax5.set_yscale('log')\n",
    "    ax5.set_xlabel('Width F')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # now correlations\n",
    "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4, figsize=(12,3))\n",
    "    plotCorr(ax0,dSet[\"trim\"],dSet[\"tMean0\"],'Mean 0')\n",
    "    plotCorr(ax1,dSet[\"trim\"],dSet[\"tMeanF\"],'Mean F')\n",
    "    plotCorr(ax2,dSet[\"trim\"],dSet[\"tWidth0\"],'Width 0')\n",
    "    plotCorr(ax3,dSet[\"trim\"],dSet[\"tWidthF\"],'Width F')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def drawSetwMask(dSet, title='Dataset', norm=False):\n",
    "    # Create a figure with 6 subplots in one row\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1, 6, figsize=(17, 3))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Plot for mask distribution\n",
    "    ax1.hist(dSet[\"mask\"], bins=50, color='gray')\n",
    "    ax1.set_xlabel(\"Mask\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # Histogram for final Trim\n",
    "    ax2.hist(dSet[\"trim\"], bins=np.arange(-0.5, 15.5, 1), color='C0')\n",
    "    ax2.set_xlabel('Final Trim')\n",
    "    \n",
    "    # Plot for tMean0\n",
    "    if norm:\n",
    "        bins = np.linspace(-2, 2, 128)\n",
    "    else:\n",
    "        bins = np.linspace(-0.5, 2047.5, 128)\n",
    "    ax3.hist(dSet[\"tMean0\"], bins=bins, label=\"0 Mean\", color='C1')\n",
    "    ax3.legend()\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set_xlabel('Mean 0')\n",
    "    \n",
    "    # Plot for tMeanF\n",
    "    ax4.hist(dSet[\"tMeanF\"], bins=bins, label=\"F Mean\", color='C2')\n",
    "    ax4.legend()\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.set_xlabel('Mean F')\n",
    "    \n",
    "    # For widths, adjust bins based on normalization flag\n",
    "    if norm:\n",
    "        bins = np.linspace(-2, 2, 128)\n",
    "    else:\n",
    "        bins = np.linspace(-0.5, 25.5, 26)\n",
    "    ax5.hist(dSet[\"tWidth0\"], bins=bins, label=\"0 Width\", color='C3')\n",
    "    ax5.legend()\n",
    "    ax5.set_yscale('log')\n",
    "    ax5.set_xlabel('Width 0')\n",
    "    \n",
    "    ax6.hist(dSet[\"tWidthF\"], bins=bins, label=\"F Width\", color='C4')\n",
    "    ax6.legend()\n",
    "    ax6.set_yscale('log')\n",
    "    ax6.set_xlabel('Width F')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Now the correlation plots (2D histograms) for each feature vs. trim\n",
    "    fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    plotCorr(ax0, dSet[\"trim\"], dSet[\"tMean0\"], 'Mean 0')\n",
    "    plotCorr(ax1, dSet[\"trim\"], dSet[\"tMeanF\"], 'Mean F')\n",
    "    plotCorr(ax2, dSet[\"trim\"], dSet[\"tWidth0\"], 'Width 0')\n",
    "    plotCorr(ax3, dSet[\"trim\"], dSet[\"tWidthF\"], 'Width F')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and normalized training data for: ('equalisation_N030_A7', 'VP0-1')\n",
      "Loaded and normalized training data for: ('equalisation_M59_A4', 'VP0-0')\n",
      "Loaded and normalized training data for: ('equalisation_M98_A1', 'VP0-1')\n",
      "Loaded and normalized training data for: ('equalisation_N030_A7', 'VP3-2')\n",
      "Loaded and normalized training data for: ('equalisation_M98_A1', 'VP2-0')\n",
      "Initial MSE=82.9346, MAE=8.7728\n",
      "Epoch 1/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790us/step - loss: 79.0720 - mae: 8.5442\n",
      "Epoch 2/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 66.6930 - mae: 7.7836\n",
      "Epoch 3/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 58.3238 - mae: 7.2215\n",
      "Epoch 4/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 50.2419 - mae: 6.6387\n",
      "Epoch 5/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 38.5892 - mae: 5.7082\n",
      "Epoch 6/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 25.1375 - mae: 4.4241\n",
      "Epoch 7/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 14.2570 - mae: 3.1328\n",
      "Epoch 8/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 8.7434 - mae: 2.3716\n",
      "Epoch 9/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 6.4764 - mae: 2.0413\n",
      "Epoch 10/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 5.1513 - mae: 1.8244\n",
      "Epoch 11/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 4.0163 - mae: 1.6014\n",
      "Epoch 12/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 3.1584 - mae: 1.4114\n",
      "Epoch 13/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5103 - mae: 1.2445\n",
      "Epoch 14/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 2.0838 - mae: 1.1212\n",
      "Epoch 15/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 1.7665 - mae: 1.0236\n",
      "Epoch 16/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 1.5524 - mae: 0.9540\n",
      "Epoch 17/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 1.4073 - mae: 0.9033\n",
      "Epoch 18/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 1.3161 - mae: 0.8712\n",
      "Epoch 19/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - loss: 1.2367 - mae: 0.8435\n",
      "Epoch 20/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 1.1791 - mae: 0.8263\n",
      "Epoch 21/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 1.1402 - mae: 0.8104\n",
      "Epoch 22/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 1.1154 - mae: 0.8007\n",
      "Epoch 23/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 1.0835 - mae: 0.7909\n",
      "Epoch 24/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 738us/step - loss: 1.0493 - mae: 0.7803\n",
      "Epoch 25/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - loss: 1.0311 - mae: 0.7722\n",
      "Epoch 26/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 1.0022 - mae: 0.7631\n",
      "Epoch 27/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.9773 - mae: 0.7532\n",
      "Epoch 28/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - loss: 0.9630 - mae: 0.7454\n",
      "Epoch 29/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.9315 - mae: 0.7365\n",
      "Epoch 30/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 0.9181 - mae: 0.7303\n",
      "Epoch 31/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.8941 - mae: 0.7221\n",
      "Epoch 32/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 0.8603 - mae: 0.7099\n",
      "Epoch 33/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.8392 - mae: 0.7014\n",
      "Epoch 34/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 0.8189 - mae: 0.6938\n",
      "Epoch 35/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.7882 - mae: 0.6818\n",
      "Epoch 36/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.7684 - mae: 0.6736\n",
      "Epoch 37/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 749us/step - loss: 0.7485 - mae: 0.6648\n",
      "Epoch 38/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.7292 - mae: 0.6568 \n",
      "Epoch 39/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6980 - mae: 0.6451\n",
      "Epoch 40/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.6833 - mae: 0.6373\n",
      "Epoch 41/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.6585 - mae: 0.6276\n",
      "Epoch 42/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - loss: 0.6362 - mae: 0.6181\n",
      "Epoch 43/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.6211 - mae: 0.6106\n",
      "Epoch 44/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 0.5947 - mae: 0.5995\n",
      "Epoch 45/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.5727 - mae: 0.5893\n",
      "Epoch 46/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.5577 - mae: 0.5823\n",
      "Epoch 47/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.5377 - mae: 0.5717\n",
      "Epoch 48/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.5227 - mae: 0.5647\n",
      "Epoch 49/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.5079 - mae: 0.5565\n",
      "Epoch 50/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.4880 - mae: 0.5465\n",
      "Epoch 51/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4760 - mae: 0.5399\n",
      "Epoch 52/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.4658 - mae: 0.5354\n",
      "Epoch 53/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.4579 - mae: 0.5277\n",
      "Epoch 54/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.4492 - mae: 0.5250\n",
      "Epoch 55/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.4379 - mae: 0.5183\n",
      "Epoch 56/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.4355 - mae: 0.5171\n",
      "Epoch 57/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.4280 - mae: 0.5137\n",
      "Epoch 58/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.4174 - mae: 0.5077\n",
      "Epoch 59/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - loss: 0.4140 - mae: 0.5060\n",
      "Epoch 60/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.4141 - mae: 0.5074\n",
      "Epoch 61/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.4081 - mae: 0.5043\n",
      "Epoch 62/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4087 - mae: 0.5048\n",
      "Epoch 63/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687us/step - loss: 0.4037 - mae: 0.5031\n",
      "Epoch 64/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.4044 - mae: 0.5023\n",
      "Epoch 65/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.4040 - mae: 0.5026\n",
      "Epoch 66/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 0.4005 - mae: 0.5013\n",
      "Epoch 67/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.4010 - mae: 0.5018\n",
      "Epoch 68/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.4000 - mae: 0.5014\n",
      "Epoch 69/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.3967 - mae: 0.5000\n",
      "Epoch 70/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.3952 - mae: 0.4989\n",
      "Epoch 71/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.3965 - mae: 0.5000\n",
      "Epoch 72/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.3917 - mae: 0.4970\n",
      "Epoch 73/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3922 - mae: 0.4980\n",
      "Epoch 74/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.3912 - mae: 0.4965\n",
      "Epoch 75/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.3947 - mae: 0.4984\n",
      "Epoch 76/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3844 - mae: 0.4935\n",
      "Epoch 77/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - loss: 0.3882 - mae: 0.4953\n",
      "Epoch 78/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 0.3899 - mae: 0.4962\n",
      "Epoch 79/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.3863 - mae: 0.4936\n",
      "Epoch 80/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.3862 - mae: 0.4932\n",
      "Epoch 81/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.3814 - mae: 0.4917\n",
      "Epoch 82/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.3825 - mae: 0.4919\n",
      "Epoch 83/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.3826 - mae: 0.4915\n",
      "Epoch 84/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3815 - mae: 0.4913\n",
      "Epoch 85/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.3816 - mae: 0.4905\n",
      "Epoch 86/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596us/step - loss: 0.3796 - mae: 0.4886\n",
      "Epoch 87/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 0.3793 - mae: 0.4890\n",
      "Epoch 88/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - loss: 0.3812 - mae: 0.4894\n",
      "Epoch 89/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.3746 - mae: 0.4866\n",
      "Epoch 90/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.3773 - mae: 0.4881\n",
      "Epoch 91/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695us/step - loss: 0.3750 - mae: 0.4860\n",
      "Epoch 92/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.3775 - mae: 0.4875\n",
      "Epoch 93/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.3758 - mae: 0.4861\n",
      "Epoch 94/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3763 - mae: 0.4870\n",
      "Epoch 95/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3746 - mae: 0.4852\n",
      "Epoch 96/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 0.3738 - mae: 0.4855\n",
      "Epoch 97/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.3734 - mae: 0.4844\n",
      "Epoch 98/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.3713 - mae: 0.4840\n",
      "Epoch 99/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.3742 - mae: 0.4852\n",
      "Epoch 100/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.3717 - mae: 0.4843\n",
      "Epoch 101/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - loss: 0.3728 - mae: 0.4845\n",
      "Epoch 102/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.3725 - mae: 0.4838\n",
      "Epoch 103/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.3751 - mae: 0.4828\n",
      "Epoch 104/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.3701 - mae: 0.4821\n",
      "Epoch 105/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3702 - mae: 0.4821\n",
      "Epoch 106/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732us/step - loss: 0.3688 - mae: 0.4827\n",
      "Epoch 107/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3683 - mae: 0.4816\n",
      "Epoch 108/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 0.3677 - mae: 0.4806\n",
      "Epoch 109/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.3721 - mae: 0.4818\n",
      "Epoch 110/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 0.3661 - mae: 0.4800\n",
      "Epoch 111/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 0.3672 - mae: 0.4803\n",
      "Epoch 112/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 0.3662 - mae: 0.4787\n",
      "Epoch 113/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 0.3680 - mae: 0.4809\n",
      "Epoch 114/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.3657 - mae: 0.4797\n",
      "Epoch 115/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3668 - mae: 0.4795\n",
      "Epoch 116/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.3641 - mae: 0.4779\n",
      "Epoch 117/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step - loss: 0.3664 - mae: 0.4782\n",
      "Epoch 118/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.3646 - mae: 0.4786\n",
      "Epoch 119/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 0.3650 - mae: 0.4771\n",
      "Epoch 120/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.3660 - mae: 0.4786\n",
      "Epoch 121/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.3602 - mae: 0.4760\n",
      "Epoch 122/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - loss: 0.3625 - mae: 0.4761\n",
      "Epoch 123/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.3652 - mae: 0.4771\n",
      "Epoch 124/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.3618 - mae: 0.4764\n",
      "Epoch 125/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3621 - mae: 0.4761\n",
      "Epoch 126/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.3600 - mae: 0.4751\n",
      "Epoch 127/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.3615 - mae: 0.4756\n",
      "Epoch 128/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.3574 - mae: 0.4731\n",
      "Epoch 129/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.3576 - mae: 0.4730\n",
      "Epoch 130/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.3524 - mae: 0.4712\n",
      "Epoch 131/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3554 - mae: 0.4717\n",
      "Epoch 132/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.3576 - mae: 0.4738\n",
      "Epoch 133/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.3558 - mae: 0.4720\n",
      "Epoch 134/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.3536 - mae: 0.4703\n",
      "Epoch 135/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.3547 - mae: 0.4717\n",
      "Epoch 136/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.3568 - mae: 0.4715\n",
      "Epoch 137/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.3527 - mae: 0.4696\n",
      "Epoch 138/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.3541 - mae: 0.4714\n",
      "Epoch 139/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - loss: 0.3503 - mae: 0.4694\n",
      "Epoch 140/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step - loss: 0.3520 - mae: 0.4694\n",
      "Epoch 141/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.3519 - mae: 0.4698\n",
      "Epoch 142/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3542 - mae: 0.4707\n",
      "Epoch 143/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 0.3489 - mae: 0.4684\n",
      "Epoch 144/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.3535 - mae: 0.4711\n",
      "Epoch 145/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.3493 - mae: 0.4673\n",
      "Epoch 146/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.3452 - mae: 0.4660\n",
      "Epoch 147/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 0.3456 - mae: 0.4666\n",
      "Epoch 148/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 0.3503 - mae: 0.4676\n",
      "Epoch 149/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step - loss: 0.3462 - mae: 0.4667\n",
      "Epoch 150/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3486 - mae: 0.4682\n",
      "Epoch 151/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.3469 - mae: 0.4667\n",
      "Epoch 152/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.3467 - mae: 0.4669\n",
      "Epoch 153/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.3441 - mae: 0.4658\n",
      "Epoch 154/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 0.3509 - mae: 0.4669\n",
      "Epoch 155/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3467 - mae: 0.4662\n",
      "Epoch 156/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.3475 - mae: 0.4654\n",
      "Epoch 157/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.3445 - mae: 0.4658\n",
      "Epoch 158/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 0.3462 - mae: 0.4657\n",
      "Epoch 159/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.3462 - mae: 0.4654\n",
      "Epoch 160/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.3474 - mae: 0.4659\n",
      "Epoch 161/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - loss: 0.3449 - mae: 0.4641\n",
      "Epoch 162/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.3458 - mae: 0.4643\n",
      "Epoch 163/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 0.3452 - mae: 0.4653\n",
      "Epoch 164/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3472 - mae: 0.4647\n",
      "Epoch 165/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 0.3454 - mae: 0.4650\n",
      "Epoch 166/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - loss: 0.3458 - mae: 0.4647\n",
      "Epoch 167/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - loss: 0.3474 - mae: 0.4647\n",
      "Epoch 168/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - loss: 0.3462 - mae: 0.4641\n",
      "Epoch 169/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.3449 - mae: 0.4648\n",
      "Epoch 170/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - loss: 0.3449 - mae: 0.4636\n",
      "Epoch 171/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 0.3449 - mae: 0.4640\n",
      "Epoch 172/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.3434 - mae: 0.4636\n",
      "Epoch 173/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.3424 - mae: 0.4627\n",
      "Epoch 174/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3428 - mae: 0.4637  \n",
      "Epoch 175/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 0.3420 - mae: 0.4627\n",
      "Epoch 176/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3432 - mae: 0.4632\n",
      "Epoch 177/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3473 - mae: 0.4640\n",
      "Epoch 178/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 0.3422 - mae: 0.4623\n",
      "Epoch 179/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3424 - mae: 0.4630\n",
      "Epoch 180/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 0.3452 - mae: 0.4645\n",
      "Epoch 181/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.3435 - mae: 0.4633\n",
      "Epoch 182/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.3435 - mae: 0.4629\n",
      "Epoch 183/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3442 - mae: 0.4634\n",
      "Epoch 184/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.3452 - mae: 0.4623\n",
      "Epoch 185/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.3482 - mae: 0.4642\n",
      "Epoch 186/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 0.3433 - mae: 0.4627 \n",
      "Epoch 187/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 0.3440 - mae: 0.4636\n",
      "Epoch 188/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 0.3431 - mae: 0.4623\n",
      "Epoch 189/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3432 - mae: 0.4623\n",
      "Epoch 190/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - loss: 0.3412 - mae: 0.4619\n",
      "Epoch 191/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3409 - mae: 0.4616\n",
      "Epoch 192/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 0.3448 - mae: 0.4626\n",
      "Epoch 193/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - loss: 0.3426 - mae: 0.4623\n",
      "Epoch 194/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3406 - mae: 0.4607\n",
      "Epoch 195/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 0.3391 - mae: 0.4608\n",
      "Epoch 196/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 0.3421 - mae: 0.4614\n",
      "Epoch 197/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.3405 - mae: 0.4618\n",
      "Epoch 198/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.3427 - mae: 0.4611\n",
      "Epoch 199/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.3448 - mae: 0.4625\n",
      "Epoch 200/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3392 - mae: 0.4599\n",
      "Epoch 201/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - loss: 0.3393 - mae: 0.4602\n",
      "Epoch 202/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 0.3399 - mae: 0.4602\n",
      "Epoch 203/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.3423 - mae: 0.4616\n",
      "Epoch 204/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.3425 - mae: 0.4615\n",
      "Epoch 205/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.3427 - mae: 0.4619\n",
      "Epoch 206/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.3432 - mae: 0.4622\n",
      "Epoch 207/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - loss: 0.3413 - mae: 0.4615\n",
      "Epoch 208/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3412 - mae: 0.4607\n",
      "Epoch 209/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.3437 - mae: 0.4626\n",
      "Epoch 210/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 0.3413 - mae: 0.4610\n",
      "Epoch 211/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 0.3403 - mae: 0.4611\n",
      "Epoch 212/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.3396 - mae: 0.4608\n",
      "Epoch 213/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3407 - mae: 0.4602 \n",
      "Epoch 214/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3423 - mae: 0.4620\n",
      "Epoch 215/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.3403 - mae: 0.4606\n",
      "Epoch 216/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 0.3407 - mae: 0.4611\n",
      "Epoch 217/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.3401 - mae: 0.4601\n",
      "Epoch 218/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.3385 - mae: 0.4607\n",
      "Epoch 219/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906us/step - loss: 0.3417 - mae: 0.4614\n",
      "Epoch 220/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3392 - mae: 0.4597\n",
      "Epoch 221/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.3397 - mae: 0.4601\n",
      "Epoch 222/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 0.3396 - mae: 0.4599\n",
      "Epoch 223/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.3396 - mae: 0.4608\n",
      "Epoch 224/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3401 - mae: 0.4607  \n",
      "Epoch 225/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.3415 - mae: 0.4605\n",
      "Epoch 226/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.3394 - mae: 0.4595\n",
      "Epoch 227/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736us/step - loss: 0.3415 - mae: 0.4615\n",
      "Epoch 228/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - loss: 0.3391 - mae: 0.4599\n",
      "Epoch 229/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 0.3414 - mae: 0.4613\n",
      "Epoch 230/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 920us/step - loss: 0.3398 - mae: 0.4612\n",
      "Epoch 231/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 0.3390 - mae: 0.4601\n",
      "Epoch 232/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 0.3394 - mae: 0.4602\n",
      "Epoch 233/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.3408 - mae: 0.4601\n",
      "Epoch 234/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - loss: 0.3374 - mae: 0.4585\n",
      "Epoch 235/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 0.3379 - mae: 0.4597\n",
      "Epoch 236/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3403 - mae: 0.4606\n",
      "Epoch 237/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 0.3426 - mae: 0.4604\n",
      "Epoch 238/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 0.3415 - mae: 0.4597\n",
      "Epoch 239/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.3404 - mae: 0.4590\n",
      "Epoch 240/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3403 - mae: 0.4597 \n",
      "Epoch 241/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.3393 - mae: 0.4596\n",
      "Epoch 242/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - loss: 0.3410 - mae: 0.4605\n",
      "Epoch 243/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - loss: 0.3391 - mae: 0.4586\n",
      "Epoch 244/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3391 - mae: 0.4601\n",
      "Epoch 245/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.3414 - mae: 0.4600\n",
      "Epoch 246/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - loss: 0.3388 - mae: 0.4593\n",
      "Epoch 247/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.3395 - mae: 0.4596\n",
      "Epoch 248/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 0.3387 - mae: 0.4588\n",
      "Epoch 249/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 876us/step - loss: 0.3398 - mae: 0.4612\n",
      "Epoch 250/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.3379 - mae: 0.4581\n",
      "Epoch 251/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 0.3384 - mae: 0.4589\n",
      "Epoch 252/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.3388 - mae: 0.4594\n",
      "Epoch 253/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.3379 - mae: 0.4580\n",
      "Epoch 254/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3388 - mae: 0.4584\n",
      "Epoch 255/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 0.3385 - mae: 0.4591\n",
      "Epoch 256/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 731us/step - loss: 0.3401 - mae: 0.4597\n",
      "Epoch 257/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.3355 - mae: 0.4573\n",
      "Epoch 258/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 0.3405 - mae: 0.4604\n",
      "Epoch 259/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.3372 - mae: 0.4583\n",
      "Epoch 260/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.3398 - mae: 0.4594\n",
      "Epoch 261/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - loss: 0.3420 - mae: 0.4610\n",
      "Epoch 262/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - loss: 0.3389 - mae: 0.4590\n",
      "Epoch 263/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3393 - mae: 0.4599\n",
      "Epoch 264/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 0.3399 - mae: 0.4595\n",
      "Epoch 265/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 756us/step - loss: 0.3357 - mae: 0.4578\n",
      "Epoch 266/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.3414 - mae: 0.4595\n",
      "Epoch 267/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.3382 - mae: 0.4596\n",
      "Epoch 268/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3399 - mae: 0.4590\n",
      "Epoch 269/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 0.3393 - mae: 0.4591\n",
      "Epoch 270/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - loss: 0.3377 - mae: 0.4588\n",
      "Epoch 271/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step - loss: 0.3386 - mae: 0.4584\n",
      "Epoch 272/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961us/step - loss: 0.3358 - mae: 0.4584\n",
      "Epoch 273/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3383 - mae: 0.4588\n",
      "Epoch 274/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 0.3399 - mae: 0.4598\n",
      "Epoch 275/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3370 - mae: 0.4578\n",
      "Epoch 276/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3383 - mae: 0.4603\n",
      "Epoch 277/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - loss: 0.3373 - mae: 0.4584\n",
      "Epoch 278/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.3370 - mae: 0.4593\n",
      "Epoch 279/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - loss: 0.3390 - mae: 0.4589\n",
      "Epoch 280/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3356 - mae: 0.4573\n",
      "Epoch 281/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.3397 - mae: 0.4591\n",
      "Epoch 282/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 0.3381 - mae: 0.4582\n",
      "Epoch 283/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.3395 - mae: 0.4592\n",
      "Epoch 284/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 0.3392 - mae: 0.4593\n",
      "Epoch 285/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.3384 - mae: 0.4589\n",
      "Epoch 286/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 0.3415 - mae: 0.4598\n",
      "Epoch 287/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step - loss: 0.3390 - mae: 0.4586\n",
      "Epoch 288/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 0.3363 - mae: 0.4574\n",
      "Epoch 289/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.3396 - mae: 0.4589\n",
      "Epoch 290/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3386 - mae: 0.4589\n",
      "Epoch 291/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3379 - mae: 0.4587\n",
      "Epoch 292/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - loss: 0.3378 - mae: 0.4585\n",
      "Epoch 293/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 0.3402 - mae: 0.4603\n",
      "Epoch 294/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 0.3385 - mae: 0.4580\n",
      "Epoch 295/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 0.3357 - mae: 0.4574\n",
      "Epoch 296/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 0.3372 - mae: 0.4581\n",
      "Epoch 297/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3395 - mae: 0.4583\n",
      "Epoch 298/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.3390 - mae: 0.4587\n",
      "Epoch 299/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 0.3369 - mae: 0.4577\n",
      "Epoch 300/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 0.3363 - mae: 0.4576\n",
      "Epoch 301/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 0.3379 - mae: 0.4585\n",
      "Epoch 302/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3384 - mae: 0.4585\n",
      "Epoch 303/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3376 - mae: 0.4583  \n",
      "Epoch 304/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714us/step - loss: 0.3402 - mae: 0.4589\n",
      "Epoch 305/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step - loss: 0.3364 - mae: 0.4573\n",
      "Epoch 306/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step - loss: 0.3382 - mae: 0.4583\n",
      "Epoch 307/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 0.3387 - mae: 0.4590\n",
      "Epoch 308/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 0.3391 - mae: 0.4592\n",
      "Epoch 309/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3395 - mae: 0.4593\n",
      "Epoch 310/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 0.3371 - mae: 0.4577\n",
      "Epoch 311/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - loss: 0.3366 - mae: 0.4578\n",
      "Epoch 312/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 0.3370 - mae: 0.4572\n",
      "Epoch 313/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 0.3375 - mae: 0.4584\n",
      "Epoch 314/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 0.3389 - mae: 0.4593\n",
      "Epoch 315/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 0.3352 - mae: 0.4560\n",
      "Epoch 316/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - loss: 0.3397 - mae: 0.4589\n",
      "Epoch 317/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 0.3368 - mae: 0.4580\n",
      "Epoch 318/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 702us/step - loss: 0.3374 - mae: 0.4570\n",
      "Epoch 319/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - loss: 0.3394 - mae: 0.4583\n",
      "Epoch 320/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.3366 - mae: 0.4581\n",
      "Epoch 321/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 0.3354 - mae: 0.4581\n",
      "Epoch 322/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 0.3381 - mae: 0.4582\n",
      "Epoch 323/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - loss: 0.3363 - mae: 0.4571\n",
      "Epoch 324/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3376 - mae: 0.4581\n",
      "Epoch 325/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 0.3396 - mae: 0.4597\n",
      "Epoch 326/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 759us/step - loss: 0.3356 - mae: 0.4572\n",
      "Epoch 327/10000\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 0.3388 - mae: 0.4588\n",
      "Training completed in 28.96 seconds\n",
      "Final   MSE=0.3400, MAE=0.4593\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "trainSets = []\n",
    "for dSet in trainingSet:\n",
    "    try:\n",
    "        outSet = extractFromTGZ(tgzFileName, dSet)\n",
    "        outSet_norm = normSet(outSet)\n",
    "        trainSets.append(outSet_norm)\n",
    "        print(\"Loaded and normalized training data for:\", dSet)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading training data for\", dSet, \":\", e)\n",
    "\n",
    "# Initialize a dictionary to store combined training data\n",
    "combined_trainSet = {\n",
    "    \"tMean0\": [],\n",
    "    \"tMeanF\": [],\n",
    "    \"tWidth0\": [],\n",
    "    \"tWidthF\": [],\n",
    "    \"trim\": []\n",
    "}\n",
    "\n",
    "# Merge all normalized datasets into one\n",
    "for trainSet in trainSets:\n",
    "    for key in combined_trainSet.keys():\n",
    "        combined_trainSet[key].extend(trainSet[key])  # Append each dataset\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "for key in combined_trainSet.keys():\n",
    "    combined_trainSet[key] = np.array(combined_trainSet[key])\n",
    "\n",
    "def buildModel_regression(nVal, nLayer, nUnits):\n",
    "    inputs = Input(shape=(nVal,))\n",
    "    x = Dense(nUnits)(inputs)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    for _ in range(nLayer - 1):\n",
    "        x = Dense(nUnits)(x)\n",
    "        x = LeakyReLU(0.1)(x)\n",
    "    output = Dense(1)(x)\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def trainNN_regression(dSet, nLayer, nUnits, saveFile=None):\n",
    "    # extract features & target\n",
    "    x_all = np.column_stack([\n",
    "        dSet[\"tMean0\"], dSet[\"tMeanF\"],\n",
    "        dSet[\"tWidth0\"], dSet[\"tWidthF\"]\n",
    "    ])\n",
    "    y_all = dSet[\"trim\"].astype(float)\n",
    "\n",
    "    # split even/odd for train/eval\n",
    "    x_train, y_train = x_all[::2], y_all[::2]\n",
    "    x_eval,  y_eval  = x_all[1::2], y_all[1::2]\n",
    "\n",
    "    model = buildModel_regression(x_all.shape[1], nLayer, nUnits)\n",
    "\n",
    "    # initial eval\n",
    "    init = model.evaluate(x_eval, y_eval, verbose=0)\n",
    "    print(f\"Initial MSE={init[0]:.4f}, MAE={init[1]:.4f}\")\n",
    "\n",
    "    # train\n",
    "    es = EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=2048,\n",
    "        epochs=10000,\n",
    "        callbacks=[es],\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    # final eval\n",
    "    final = model.evaluate(x_eval, y_eval, verbose=0)\n",
    "    print(f\"Final   MSE={final[0]:.4f}, MAE={final[1]:.4f}\")\n",
    "\n",
    "    # plot loss & mae\n",
    "    plt.plot(history.history[\"loss\"], label=\"MSE\")\n",
    "    plt.plot(history.history[\"mae\"],  label=\"MAE\")\n",
    "    plt.xlabel(\"Epoch\"); plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    if saveFile:\n",
    "        model.save(saveFile, overwrite=True)\n",
    "    return model\n",
    "\n",
    "# --- run it on your combined_trainSet dict ---\n",
    "saveFile = f\"NN_{trainingSet[0][0]}_{trainingSet[0][1]}_regression.keras\"\n",
    "model = trainNN_regression(combined_trainSet, nLayer=2, nUnits=4, saveFile=saveFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 470us/step\n",
      "\u001b[1m2041/2041\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 461us/step\n",
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 471us/step\n",
      "\u001b[1m2047/2047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 451us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 476us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m2038/2038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 488us/step\n",
      "\u001b[1m2034/2034\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478us/step\n",
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 467us/step\n",
      "\u001b[1m2046/2046\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 466us/step\n",
      "\u001b[1m2036/2036\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 449us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 472us/step\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 457us/step\n",
      "\u001b[1m2037/2037\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462us/step\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452us/step\n",
      "\u001b[1m2042/2042\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 469us/step\n",
      "\u001b[1m2040/2040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 453us/step\n",
      "\u001b[1m2044/2044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 452us/step\n",
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462us/step\n",
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 475us/step\n",
      "\u001b[1m2040/2040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 498us/step\n",
      "\u001b[1m2037/2037\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 478us/step\n",
      "\u001b[1m2039/2039\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 484us/step\n",
      "\u001b[1m2045/2045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 463us/step\n",
      "\u001b[1m2040/2040\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 490us/step\n",
      "\u001b[1m2043/2043\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462us/step\n",
      "Saved regression ±1 adjacent accuracies to 'regression_adjacc_results_tune.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_and_prep(dSet, extractor, normalise=True):\n",
    "    out = extractor(tgzFileName, dSet)\n",
    "    out = removeMaskNaN(out)\n",
    "    if normalise:\n",
    "        out = normSet(out)\n",
    "    x = np.column_stack([\n",
    "        out[\"tMean0\"], out[\"tMeanF\"],\n",
    "        out[\"tWidth0\"], out[\"tWidthF\"]\n",
    "    ])\n",
    "    y = out[\"trim\"].astype(int)\n",
    "    return x, y\n",
    "\n",
    "def adjacent_accuracy(y_true, y_pred_cont, tol=1):\n",
    "    y_int = np.clip(np.round(y_pred_cont), 0, 15).astype(int)\n",
    "    return np.mean(np.abs(y_int - y_true) <= tol)\n",
    "\n",
    "# load your saved regression model\n",
    "reg = load_model(saveFile, compile=False)\n",
    "\n",
    "import random\n",
    "\n",
    "random_eval_sets = random.sample(evaluationSet, 30)\n",
    "\n",
    "results = []\n",
    "for dSet in random_eval_sets:\n",
    "    x_eval, y_eval = load_and_prep(dSet, extractFromTGZ, normalise=True)\n",
    "    y_pred = reg.predict(x_eval).flatten()\n",
    "    acc = adjacent_accuracy(y_eval, y_pred, tol=1)\n",
    "    results.append({\n",
    "        \"Dataset\": f\"{dSet[0]}-{dSet[1]}\",\n",
    "        \"AdjAcc_reg\": f\"{acc:.4f}\",\n",
    "        \"Batch Size\": \"2048\"\n",
    "    })\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv_file = \"regression_adjacc_results_tune.csv\"\n",
    "fieldnames = [\"Dataset\",\"AdjAcc_reg\",\"Batch Size\"]\n",
    "\n",
    "# detect whether we need to write the header\n",
    "write_header = not os.path.exists(csv_file)\n",
    "\n",
    "with open(csv_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Saved regression ±1 adjacent accuracies to '{csv_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 652us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step\n",
      "Appended 12 Module25 results to 'regression_adjacc_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import os, csv\n",
    "\n",
    "results_new = []\n",
    "for dSet in evaluationSet_new:\n",
    "    # 1) Pull & clean with the new tar\n",
    "    out = extractFromTGZ_new(tgzFileName_new, dSet)\n",
    "    out = removeMaskNaN(out)\n",
    "    out = normSet(out)\n",
    "\n",
    "    # 2) Build x, y\n",
    "    x_eval = np.column_stack([out[\"tMean0\"], out[\"tMeanF\"],\n",
    "                              out[\"tWidth0\"], out[\"tWidthF\"]])\n",
    "    y_eval = out[\"trim\"].astype(int)\n",
    "\n",
    "    # 3) Predict & score\n",
    "    y_pred = reg.predict(x_eval, batch_size=2048).flatten()\n",
    "    acc    = adjacent_accuracy(y_eval, y_pred, tol=1)\n",
    "\n",
    "    results_new.append({\n",
    "        \"Dataset\":    f\"{dSet[0]}-{dSet[1]}\",\n",
    "        \"AdjAcc_reg\": f\"{acc:.4f}\",\n",
    "    })\n",
    "\n",
    "# 4) Append to the same CSV\n",
    "csv_file   = \"regression_adjacc_results.csv\"\n",
    "fieldnames = [\"Dataset\",\"AdjAcc_reg\",\"Batch Size\"]\n",
    "write_hdr  = not os.path.exists(csv_file)\n",
    "\n",
    "with open(csv_file, \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    if write_hdr:\n",
    "        writer.writeheader()\n",
    "    writer.writerows(results_new)\n",
    "\n",
    "print(f\"Appended {len(results_new)} Module25 results to '{csv_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
